* More execution options

Parkour currently inherits Hadoop’s default job-failure behavior when a job’s
output directory already exists.  It would be nice to support other behaviors,
such as skipping existing, overwriting existing, or a make-like model where
outputs are overwritten when their inputs have changed.

* Integration for writing other Hadoop classes

** Input formats
** Output formats
** (Raw)Comparators
** Writables (?)
** Other things?

* Logging

- Probably other logging?

* Use class loader instead of pools

The current implementation builds small statically-compiled pools of
var-trampolining implementations of the various Hadoop classes which Hadoop
expects to locate via name-based reflection.  A more general implementation
would use a custom ClassLoader which could dynamically generate and return new
instances of the supported types.

The ClassLoader approach may not however be possible.  It’s not clear if Hadoop
allows tasks to configure an alternative ClassLoader prior to task
initialization.

* Make dseqs locally foldable

Right now dseqs may be locally =reduce=-d, but there’s no reason they shouldn’t
be =fold=-able.  The fold implementation should be able to use fork-join to run
the fold in parallel across the input splits.

* Support for EMR

Probably a separate project.  Suggested by ztellman, potentially a Leiningen
plugin, mechanism for easily spinning up EMR clusters and launching Parkour
jobs.

* Cluster-REPL integration

Probably also a separate project, but potentially with helper hooks.  Like
pdns-lab, but as an all-in-one package.  Launch nREPL server under cluster
configuration.  Leiningen integration for automatic uberjar re-building, fast
uberjar re-builds.  Use project configuration to specify job JAR.  Helper
integration for extracting and running over subset of data, as per Pig
ILLUSTRATE.  Allow in-REPL testing by rebinding to local configurations.

* Error messages

** Errors on repeated reduction (?)

Attempting to reduce a task input collection multiple times is probably almost
always a mistake.  Right now reduction of an exhausted input collection acts as
reduction on an empty collection.  Perhaps it should raise an exception instead?

* Allow repeated reduction of inputs

Alternative to erroring on repeated reduction.  We could re-initialize the input
record-reader (and context) for the original input split, allowing the same data
to be subsequently re-processed.  Not certain this is a useful feature, but is
an interesting idea.
