* More execution options

Parkour currently inherits Hadoop’s default job-failure behavior when a job’s
output directory already exists.  It would be nice to support other behaviors,
such as skipping existing, overwriting existing, or a make-like model where
outputs are overwritten when their inputs have changed.

* Integration for writing other Hadoop classes

** Input formats
** Output formats
** (Raw)Comparators
** Writables (?)
** Other things?

* Logging

- Probably other logging?

* Use class loader instead of pools

The current implementation builds small statically-compiled pools of
var-trampolining implementations of the various Hadoop classes which Hadoop
expects to locate via name-based reflection.  A more general implementation
would use a custom ClassLoader which could dynamically generate and return new
instances of the supported types.

The ClassLoader approach may not however be possible.  It’s not clear if Hadoop
allows tasks to configure an alternative ClassLoader prior to task
initialization.

* Make dseqs locally foldable

Right now dseqs may be locally =reduce=-d, but there’s no reason they shouldn’t
be =fold=-able.  The fold implementation should be able to use fork-join to run
the fold in parallel across the input splits.

* Support for EMR

Probably a separate project.  Suggested by ztellman, potentially a Leiningen
plugin, mechanism for easily spinning up EMR clusters and launching Parkour
jobs.

* Error messages

** Errors on repeated reduction (?)

Attempting to reduce a task input collection multiple times is probably almost
always a mistake.  Right now reduction of an exhausted input collection acts as
reduction on an empty collection.  Perhaps it should raise an exception instead?

* Allow repeated reduction of inputs

Alternative to erroring on repeated reduction.  We could re-initialize the input
record-reader (and context) for the original input split, allowing the same data
to be subsequently re-processed.  Not certain this is a useful feature, but is
an interesting idea.

* Extended configuration serialization

Provide an interface which ultimately allows objects to ship themselves via the
distributed cache when they are serialized into a configuration.

* n-record dseq

A dseq which acts like NLineInputFormat, but wraps an arbitrary existing
InputFormat, distributing only <n> records per mapper (or distributing available
records across <m> mappers).

* jobconf dseq

A dseq which distributes input records via the job configuration.  Should
probably use extended configuration serialization.

* dseq input path access

Provide a multimethod for accessing a dseq's input paths.

* Provide mapreduce utility namespace

Provide a namespace of common mapreduce operations, such as a =first-p=
partitioner, a =sum-r= reducer, etc.
